{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c8fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8de79",
   "metadata": {},
   "source": [
    "What sort of text inputs can gensim handle? The input text typically comes in 3 different forms :\n",
    "1. As sentences stored in pythonâ€™s native list object\n",
    "2. As one single text file, small or large.\n",
    "3. In multiple text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fefd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d20510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38408399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The',\n",
      "  'Saudis',\n",
      "  'are',\n",
      "  'preparing',\n",
      "  'a',\n",
      "  'report',\n",
      "  'that',\n",
      "  'will',\n",
      "  'acknowledge',\n",
      "  'that'],\n",
      " ['Saudi',\n",
      "  'journalist',\n",
      "  'Jamal',\n",
      "  \"Khashoggi's\",\n",
      "  'death',\n",
      "  'was',\n",
      "  'the',\n",
      "  'result',\n",
      "  'of',\n",
      "  'an'],\n",
      " ['interrogation',\n",
      "  'that',\n",
      "  'went',\n",
      "  'wrong,',\n",
      "  'one',\n",
      "  'that',\n",
      "  'was',\n",
      "  'intended',\n",
      "  'to',\n",
      "  'lead'],\n",
      " ['to',\n",
      "  'his',\n",
      "  'abduction',\n",
      "  'from',\n",
      "  'Turkey,',\n",
      "  'according',\n",
      "  'to',\n",
      "  'two',\n",
      "  'sources.']]\n"
     ]
    }
   ],
   "source": [
    "texts = [[word for word in doc.split() ] for doc in documents]\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2994d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...>\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6643fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e112282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['One', 'source', 'says', 'the', 'report', 'will', 'likely', 'conclude', 'that'], ['the', 'operation', 'was', 'carried', 'out', 'without', 'clearance', 'and'], ['transparency', 'and', 'that', 'those', 'involved', 'will', 'be', 'held'], ['responsible.', 'One', 'of', 'the', 'sources', 'acknowledged', 'that', 'the'], ['report', 'is', 'still', 'being', 'prepared', 'and', 'cautioned', 'that'], ['things', 'could', 'change.']]\n"
     ]
    }
   ],
   "source": [
    "text_2 = [[word for word in doc.split()] for doc in documents_2]\n",
    "print(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59c45ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.add_documents(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b4e35e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'One': 33, 'conclude': 34, 'likely': 35, 'says': 36, 'source': 37, 'and': 38, 'carried': 39, 'clearance': 40, 'operation': 41, 'out': 42, 'without': 43, 'be': 44, 'held': 45, 'involved': 46, 'those': 47, 'transparency': 48, 'acknowledged': 49, 'responsible.': 50, 'sources': 51, 'being': 52, 'cautioned': 53, 'is': 54, 'prepared': 55, 'still': 56, 'change.': 57, 'could': 58, 'things': 59}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "416cc479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53075d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'saudis', 'are', 'preparing', 'report', 'that', 'will', 'acknowledge', 'that'], ['saudi', 'journalist', 'jamal', 'khashoggi', 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'turkey', 'according', 'to', 'two', 'sources']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = [simple_preprocess(doc) for doc in documents]\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13742a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1)], [(6, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)], [(5, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(21, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)]]\n"
     ]
    }
   ],
   "source": [
    "mydict = corpora.Dictionary()\n",
    "mycorpus= [mydict.doc2bow(doc, allow_update = True) for doc in tokenized_text]\n",
    "print(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ac482e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('acknowledge', 1), ('are', 1), ('preparing', 1), ('report', 1), ('saudis', 1), ('that', 2), ('the', 1), ('will', 1)], [('the', 1), ('an', 1), ('death', 1), ('jamal', 1), ('journalist', 1), ('khashoggi', 1), ('of', 1), ('result', 1), ('saudi', 1), ('was', 1)], [('that', 2), ('was', 1), ('intended', 1), ('interrogation', 1), ('lead', 1), ('one', 1), ('to', 1), ('went', 1), ('wrong', 1)], [('to', 2), ('abduction', 1), ('according', 1), ('from', 1), ('his', 1), ('sources', 1), ('turkey', 1), ('two', 1)]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = [[(mydict[id],count) for id,count in line] for line in mycorpus]\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b48de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ec12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.path = path\n",
    "        self.dictionary = dictionary\n",
    "    \n",
    "    def __iter__(self):\n",
    "        global mydict\n",
    "        \n",
    "        for line in open(self.path, encoding = 'utf-8'):\n",
    "            tokenized_list = simple_preprocess(line, deacc = True)\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update = True)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "            yield bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7d067a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 2), (29, 3), (30, 1), (31, 1), (32, 1), (33, 1), (34, 3), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 8), (44, 6), (45, 1), (46, 1), (47, 3), (48, 1), (49, 1), (50, 1), (51, 3), (52, 1), (53, 3), (54, 1), (55, 1)]\n"
     ]
    }
   ],
   "source": [
    "mydict = corpora.Dictionary()\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9b5a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to save a gensim dictionary and corpus to disk and load them back?\n",
    "mydict.save('mydict.dict')  # save dict to disk\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', bow_corpus)  # save corpus to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30a0065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 3.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 1.0), (26, 1.0), (27, 1.0), (28, 2.0), (29, 3.0), (30, 1.0), (31, 1.0), (32, 1.0), (33, 1.0), (34, 3.0), (35, 1.0), (36, 1.0), (37, 1.0), (38, 1.0), (39, 1.0), (40, 1.0), (41, 2.0), (42, 1.0), (43, 8.0), (44, 6.0), (45, 1.0), (46, 1.0), (47, 3.0), (48, 1.0), (49, 1.0), (50, 1.0), (51, 3.0), (52, 1.0), (53, 3.0), (54, 1.0), (55, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Load them back\n",
    "loaded_dict = corpora.Dictionary.load('mydict.dict')\n",
    "corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c42df987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abduction', 0.07], ['according', 0.07], ['acknowledge', 0.07], ['acknowledged', 0.07], ['an', 0.07], ['and', 0.21], ['are', 0.07], ['be', 0.07], ['being', 0.07], ['carried', 0.07], ['cautioned', 0.07], ['change', 0.07], ['clearance', 0.07], ['conclude', 0.07], ['could', 0.07], ['death', 0.07], ['from', 0.07], ['held', 0.07], ['his', 0.07], ['intended', 0.07], ['interrogation', 0.07], ['involved', 0.07], ['is', 0.07], ['jamal', 0.07], ['journalist', 0.07], ['khashoggi', 0.07], ['lead', 0.07], ['likely', 0.07], ['of', 0.14], ['one', 0.21], ['operation', 0.07], ['out', 0.07], ['prepared', 0.07], ['preparing', 0.07], ['report', 0.21], ['responsible', 0.07], ['result', 0.07], ['saudi', 0.07], ['saudis', 0.07], ['says', 0.07], ['source', 0.07], ['sources', 0.14], ['still', 0.07], ['that', 0.55], ['the', 0.42], ['things', 0.07], ['those', 0.07], ['to', 0.21], ['transparency', 0.07], ['turkey', 0.07], ['two', 0.07], ['was', 0.21], ['went', 0.07], ['will', 0.21], ['without', 0.07], ['wrong', 0.07]]\n"
     ]
    }
   ],
   "source": [
    "#How to create the TFIDF matrix (corpus) in gensim?\n",
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus, smartirs = 'ntc')\n",
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id],np.around(freq , decimals = 2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5aa5fd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 400000,\n",
       " 'file_size': 69182535,\n",
       " 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
       " 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       " 'parameters': {'dimension': 50},\n",
       " 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       " 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       " 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "  'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       " 'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       " 'file_name': 'glove-wiki-gigaword-50.gz',\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "api.info('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e754388",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "w2v_model.most_similar('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7feb037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'saudis', 'are', 'preparing', 'report', 'that', 'will', 'acknowledge', 'that']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(tokenized_text, min_count=3, threshold=10)\n",
    "# Construct bigram\n",
    "print(bigram[tokenized_text[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46fc7231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'saudis', 'are', 'preparing', 'report', 'that', 'will', 'acknowledge', 'that']\n"
     ]
    }
   ],
   "source": [
    "# Build the trigram models\n",
    "trigram = gensim.models.phrases.Phrases(bigram[tokenized_text], threshold=10)\n",
    "\n",
    "# Construct trigram\n",
    "print(trigram[bigram[tokenized_text[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55161699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "def square_numbers(nums):\n",
    "    squares = []\n",
    "    for n in nums:\n",
    "        squares.append(n*n)\n",
    "    return squares\n",
    "\n",
    "numbers = [1,2,3,4,5]\n",
    "result = square_numbers(numbers)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a7dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object square_numbers at 0x000001FF544215F0>\n"
     ]
    }
   ],
   "source": [
    "def square_numbers(nums):\n",
    "    for n in nums:\n",
    "        yield n*n\n",
    "\n",
    "numbers = [1,2,3,4,5]\n",
    "result = square_numbers(numbers)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1195aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "for s in result:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621b0a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "squares = [n*n for n in [1,2,3,4,5]]\n",
    "print(squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cca5d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "generator = (n*n for n in [1,2,3,4,5])\n",
    "print(next(generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30320e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "print(list(generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa87e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
